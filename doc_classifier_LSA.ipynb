{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "super projekt na IR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtKIGGctHcTY"
      },
      "source": [
        "# **Document classifier based on Latent Semantic Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5s4rWSJUsCr"
      },
      "source": [
        "## Preprocessing and loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DNa3GGhHGJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944ecbd8-b13a-4872-a57c-00f167d14caa"
      },
      "source": [
        "'''Downloading Reuter's dataset'''\n",
        "!wget -N http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz\n",
        "!tar zxf reuters21578.tar.gz\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-27 18:47:29--  http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz\n",
            "Resolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘reuters21578.tar.gz’ not modified on server. Omitting download.\n",
            "\n",
            "all-exchanges-strings.lc.txt\t    reut2-002.sgm  reut2-013.sgm\n",
            "all-orgs-strings.lc.txt\t\t    reut2-003.sgm  reut2-014.sgm\n",
            "all-people-strings.lc.txt\t    reut2-004.sgm  reut2-015.sgm\n",
            "all-places-strings.lc.txt\t    reut2-005.sgm  reut2-016.sgm\n",
            "all-topics-strings.lc.txt\t    reut2-006.sgm  reut2-017.sgm\n",
            "cat-descriptions_120396.txt\t    reut2-007.sgm  reut2-018.sgm\n",
            "feldman-cia-worldfactbook-data.txt  reut2-008.sgm  reut2-019.sgm\n",
            "lewis.dtd\t\t\t    reut2-009.sgm  reut2-020.sgm\n",
            "README.txt\t\t\t    reut2-010.sgm  reut2-021.sgm\n",
            "reut2-000.sgm\t\t\t    reut2-011.sgm  reuters21578.tar.gz\n",
            "reut2-001.sgm\t\t\t    reut2-012.sgm  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuaDq2TTUREg",
        "outputId": "3241c5b1-9a22-4697-b9d8-2672fdef4aa2"
      },
      "source": [
        "#loading the data\n",
        "import re\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "cwd = '/content/'\n",
        "files = os.listdir(cwd)\n",
        "\n",
        "raw_y = []  #labels\n",
        "raw_X = []  #data\n",
        "\n",
        "for file in files:\n",
        "  if file.startswith(\"reut2-\") and file.endswith(\".sgm\"): \n",
        "    with open(file, 'rb') as f:\n",
        "\n",
        "      print('Reading file: %s' % file)\n",
        "\n",
        "      soup = BeautifulSoup(f, \"html.parser\")  \n",
        "      articles = soup.find_all('reuters')\n",
        "\n",
        "      for article in articles:\n",
        "        topics = article.findAll('topics')\n",
        "        bodies = article.findAll('text')\n",
        "\n",
        "        if len(topics[0]) == 0:\n",
        "          topics_list = ''  #no topic\n",
        "        else:\n",
        "          topics_list = []\n",
        "          for topic in topics[0]:  #getting rid of the tags \n",
        "            topics_list.append((topic.text).lower())\n",
        "          topics_list = topics_list[0] #taking only the first topic\n",
        "        raw_y.append(topics_list)\n",
        "        \n",
        "        if len(bodies) == 0:\n",
        "          raw_X.append('')\n",
        "        else:\n",
        "          body = (bodies[0].text).lower()\n",
        "          body = re.sub('[^A-Za-z0-9]+', ' ', body) #getting rid of special char\n",
        "          body = re.sub(r'\\d+','nn', body)  #getting rid of numbers\n",
        "          raw_X.append(body)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading file: reut2-012.sgm\n",
            "Reading file: reut2-021.sgm\n",
            "Reading file: reut2-011.sgm\n",
            "Reading file: reut2-010.sgm\n",
            "Reading file: reut2-016.sgm\n",
            "Reading file: reut2-019.sgm\n",
            "Reading file: reut2-006.sgm\n",
            "Reading file: reut2-017.sgm\n",
            "Reading file: reut2-008.sgm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaQfUcOgSFuA",
        "outputId": "5a279a6c-7924-4d0e-df5c-c09f82a344e8"
      },
      "source": [
        "print(raw_y[:5])\n",
        "print(raw_X[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['earn', 'wheat', 'sugar', '', 'earn']\n",
            "[' republic savings and loan rsla sets dividend milwaukee wis april nn qtly div nn cts vs nn cts prior pay april nn record april nn note company s full name is republic savings and loan association of wisconsin reuter ', ' shultz ussr trip fuels talk of eep wheat offer by nelson graves reuters washington april nn speculation the united states will offer subsidized wheat to the soviet union appears to have reached a new level of intensity in the run up to secretary of state george shultz visit later this month to moscow rumors of an impending deal have coursed through wheat markets since officials from the two countries held their customary semi annual grain talks in february moscow s decision at that time to reenter the u s corn market strengthened the perception of warming farm trade prospects shultz is set to arrive in moscow april nn shultz statement two weeks ago that he would not stand in the way of a wheat subsidy offer under the export enhancement program eep coupled with the announcement of his visit to moscow was interpreted by many grain trade representatives here as a clear signal that the reagan administration was preparing an offer administration officials in and out of the u s agriculture department have been extremely tight lipped about the prospects of a subsidy offer but usda officials for the most part have abandoned the contention the proposal is dormant suggesting that an offer while not a done deal is a live possibility prominent u s grain trade representatives many of whom asked not to be identified continue to maintain that an offer to subsidize four mln tonnes of wheat is imminent others who one month ago claimed a deal was not possible are saying they would not rule one out rep pat roberts r kan yesterday went so far as to predict a subsidy offer would be made within the next ten days to two weeks aides to roberts said he had spoken to republican leaders who had been in contact with administration officials richard fritz director of international marketing at u s wheat associates said he was confident an export enhancement offer would be made by the middle of this month fritz also said he thought the value of the bonus would end up being close to the offer washington made peking earlier this year when usda approved subsidies to china of around nn dlrs per tonne on one mln tonnes of wheat some grain trade representatives say a four mln tonne wheat subsidy offer might help stimulate more soviet purchases of u s corn and open the door to u s sales of soybeans as ever one of the crucial sticking points in a wheat deal would appear to be price last summer the administration took the controversial step of offering the soviets subsidized wheat but were embarrassed when moscow spurned the proposal on the grounds that the nn dlr per tonne subsidy still left u s wheat prices far above world market prices the administration s decision to set the subsidy level up front instead of accepting bids from exporters appeared to be a means of controlling the price while attempting to dampen criticism grain trade sources said nonetheless the pricing procedure did not prevent shultz from saying the soviets were chortling because washington was offering soviet housewives cheaper grain than that available to u s housewives the conventional wisdom among grain trade representatives here is that a general warming of relations between the two countries since last summer combined with continued hard times in the u s grain belt would favor a subsidy offer in addition the ussr has made it clear it would consider buying u s wheat if it were priced more competitively however observers have not forgotten the circumstances surrounding the administration s announcement of the wheat subsidy offer last summer up until the time of the announcment congressional and industry leaders were led to believe the white house had decided to expand the export enhancement program to include not only the soviets but also a much broader list of countries instead the administration scaled back the offer to include only the soviets that last minute change of heart adds a measure of uncertainty even to the predictions of those most convinced that the administration will not now pass up the opportunity to sell four mln tonnes of wheat to the soviet union reuter ', ' kaines confirms white sugar sales to india london april nn london based sugar operator kaines ltd confirmed it sold two cargoes of white sugar to india out of an estimated overall sales total of four or five cargoes in which other brokers participated the sugar for april may and april june shipment was sold at between nn and nn dlrs a tonne cif it said reuter ', ' luxtec luxt cuts warrant exercise price sturbridge mass april nn luxtec corp said it has reduced the exercise price of its class b common stock purchase warrants to one dlr from two dlrs from today through june nn reuter ', ' thl holdings inc year jan nn net canton mass april nn oper net nn nn mln vs nn nn mln revs nn nn bilion vs nn nn billion note thl is parent to scoa industries inc acquired in a leveraged buyout in december nn reuter ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0lOVpKxlxNm"
      },
      "source": [
        "assert len(raw_y) == len(raw_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQymsCaMOWUn"
      },
      "source": [
        "## Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssisiAmoU-sl"
      },
      "source": [
        "### Document Term Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjNR9IDf9jvB"
      },
      "source": [
        "Applying Tf-idf to create Document-Term Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H43bDy1rUjp9"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQFnO7nWVU7X"
      },
      "source": [
        "vectorizer = TfidfVectorizer(analyzer='word',\n",
        "                             stop_words='english',  #filtering out stop words\n",
        "                             min_df=5, #ignore terms that have a document frequency lower than 5\n",
        "                             max_df=0.95, #ignore terms that appear in 95% of documents\n",
        "                             max_features=10000, #10000 most frequently appearing words\n",
        "                             strip_accents='ascii')\n",
        "\n",
        "X = vectorizer.fit_transform(raw_X) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N345FfOoRY5i",
        "outputId": "ccde9cd7-b680-4242-bef9-7333eceb8d3e"
      },
      "source": [
        "print(vectorizer.get_feature_names()[100:200]) \n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['acted', 'acting', 'action', 'actions', 'active', 'actively', 'actives', 'activities', 'activity', 'acton', 'acts', 'actual', 'actually', 'acute', 'ad', 'ada', 'adams', 'adapt', 'adb', 'add', 'added', 'adding', 'addition', 'additional', 'additionally', 'additions', 'address', 'addressed', 'addressing', 'adds', 'adelaide', 'adequacy', 'adequate', 'adequately', 'adhere', 'adherence', 'adhering', 'adhesives', 'adjacent', 'adjourned', 'adjust', 'adjustable', 'adjusted', 'adjusting', 'adjustment', 'adjustments', 'adjusts', 'adm', 'administered', 'administration', 'administrative', 'administrator', 'admission', 'admit', 'admitted', 'admitting', 'adobe', 'adopt', 'adopted', 'adopting', 'adoption', 'adopts', 'adr', 'ads', 'advance', 'advanced', 'advances', 'advancing', 'advantage', 'advantageous', 'advantages', 'adventure', 'adverse', 'adversely', 'advertisement', 'advertising', 'advest', 'advice', 'advise', 'advised', 'adviser', 'advisers', 'advisor', 'advisors', 'advisory', 'advocate', 'advocated', 'aeg', 'aegean', 'aegon', 'aep', 'aero', 'aeronautics', 'aerospace', 'aerospatiale', 'affair', 'affairs', 'affect', 'affected', 'affecting']\n",
            "(21578, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZHsFjvNakO2"
      },
      "source": [
        "### Singular Value Decomposition "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHf4Y4oqaq5P"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "svd = TruncatedSVD(n_components=130, random_state=1) \n",
        "lsa = svd.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wjSpNhQHzS6",
        "outputId": "ceb95fd3-f096-422c-bd46-147f31430014"
      },
      "source": [
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "for i, component in enumerate(lsa.components_):\n",
        "  terms_components = zip(terms, component)\n",
        "  sorted_terms = sorted(terms_components, key=lambda x:x[1], reverse=True)[:5]\n",
        "  temp =[t[0] for t in sorted_terms]\n",
        "  print(f'Topic {i+1}: {temp}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 1: ['vs', 'cts', 'mln', 'net', 'loss']\n",
            "\n",
            "Topic 2: ['blah', 'pct', 'says', 'said', 'dlrs']\n",
            "\n",
            "Topic 3: ['said', 'pct', 'dlrs', 'mln', 'billion']\n",
            "\n",
            "Topic 4: ['cts', 'qtly', 'div', 'record', 'prior']\n",
            "\n",
            "Topic 5: ['loss', 'profit', 'said', 'company', 'shares']\n",
            "\n",
            "Topic 6: ['shares', 'mln', 'stock', 'common', 'company']\n",
            "\n",
            "Topic 7: ['pct', 'mln', 'dlrs', 'loss', 'february']\n",
            "\n",
            "Topic 8: ['billion', 'dlrs', 'quarter', 'div', 'qtly']\n",
            "\n",
            "Topic 9: ['bank', 'billion', 'stg', 'loss', 'debt']\n",
            "\n",
            "Topic 10: ['mln', 'stg', 'tonnes', 'bank', 'div']\n",
            "\n",
            "Topic 11: ['shares', 'stg', 'stock', 'billion', 'tonnes']\n",
            "\n",
            "Topic 12: ['tonnes', 'billion', 'wheat', 'bond', 'trade']\n",
            "\n",
            "Topic 13: ['oper', 'dlrs', 'tonnes', 'share', 'stock']\n",
            "\n",
            "Topic 14: ['japan', 'trade', 'yen', 'mln', 'japanese']\n",
            "\n",
            "Topic 15: ['mln', 'debt', 'offering', 'february', 'debentures']\n",
            "\n",
            "Topic 16: ['oil', 'crude', 'prices', 'opec', 'gas']\n",
            "\n",
            "Topic 17: ['maynn', 'julnn', 'sepnn', 'sales', 'decnn']\n",
            "\n",
            "Topic 18: ['maynn', 'julnn', 'sepnn', 'untrd', 'decnn']\n",
            "\n",
            "Topic 19: ['oper', 'profit', 'stg', 'debt', 'tax']\n",
            "\n",
            "Topic 20: ['profit', 'dividend', 'debt', 'quarter', 'dollar']\n",
            "\n",
            "Topic 21: ['fed', 'federal', 'new', 'president', 'stock']\n",
            "\n",
            "Topic 22: ['stock', 'dividend', 'split', 'debt', 'stg']\n",
            "\n",
            "Topic 23: ['oper', 'february', 'contract', 'stock', 'january']\n",
            "\n",
            "Topic 24: ['sales', 'oper', 'oil', 'debt', 'shrs']\n",
            "\n",
            "Topic 25: ['president', 'chief', 'officer', 'executive', 'chairman']\n",
            "\n",
            "Topic 26: ['fed', 'offering', 'dividend', 'cts', 'contract']\n",
            "\n",
            "Topic 27: ['profit', 'japan', 'oil', 'bank', 'dividend']\n",
            "\n",
            "Topic 28: ['offering', 'february', 'bank', 'contract', 'dlr']\n",
            "\n",
            "Topic 29: ['fed', 'cts', 'reserves', 'trade', 'february']\n",
            "\n",
            "Topic 30: ['ec', 'european', 'sugar', 'community', 'february']\n",
            "\n",
            "Topic 31: ['february', 'dollar', 'debt', 'baker', 'shares']\n",
            "\n",
            "Topic 32: ['gold', 'offering', 'avg', 'shrs', 'nil']\n",
            "\n",
            "Topic 33: ['francs', 'notes', 'swiss', 'issue', 'franc']\n",
            "\n",
            "Topic 34: ['gold', 'apr', 'tax', 'nil', 'budget']\n",
            "\n",
            "Topic 35: ['april', 'week', 'securities', 'dividend', 'commission']\n",
            "\n",
            "Topic 36: ['contract', 'apr', 'dlr', 'shares', 'sales']\n",
            "\n",
            "Topic 37: ['stock', 'year', 'gm', 'revs', 'car']\n",
            "\n",
            "Topic 38: ['year', 'yen', 'debt', 'april', 'ago']\n",
            "\n",
            "Topic 39: ['gold', 'april', 'debt', 'trade', 'sales']\n",
            "\n",
            "Topic 40: ['tax', 'gold', 'canada', 'new', 'canadian']\n",
            "\n",
            "Topic 41: ['trade', 'canada', 'canadian', 'american', 'chrysler']\n",
            "\n",
            "Topic 42: ['coffee', 'prices', 'american', 'loan', 'offering']\n",
            "\n",
            "Topic 43: ['new', 'york', 'gold', 'april', 'shares']\n",
            "\n",
            "Topic 44: ['gold', 'gulf', 'quarter', 'iran', 'american']\n",
            "\n",
            "Topic 45: ['week', 'sugar', 'cts', 'avg', 'shrs']\n",
            "\n",
            "Topic 46: ['april', 'new', 'york', 'canadian', 'rate']\n",
            "\n",
            "Topic 47: ['coffee', 'bank', 'budget', 'debentures', 'june']\n",
            "\n",
            "Topic 48: ['nil', 'shares', 'china', 'ec', 'revs']\n",
            "\n",
            "Topic 49: ['new', 'treasury', 'year', 'baker', 'ec']\n",
            "\n",
            "Topic 50: ['francs', 'american', 'debt', 'quarter', 'french']\n",
            "\n",
            "Topic 51: ['june', 'sugar', 'mths', 'july', 'debt']\n",
            "\n",
            "Topic 52: ['june', 'canadian', 'west', 'july', 'canada']\n",
            "\n",
            "Topic 53: ['coffee', 'net', 'reagan', 'market', 'dividend']\n",
            "\n",
            "Topic 54: ['june', 'july', 'nnst', 'union', 'tax']\n",
            "\n",
            "Topic 55: ['american', 'banks', 'tax', 'share', 'marks']\n",
            "\n",
            "Topic 56: ['credit', 'rate', 'dlr', 'american', 'group']\n",
            "\n",
            "Topic 57: ['china', 'sugar', 'year', 'loan', 'savings']\n",
            "\n",
            "Topic 58: ['split', 'american', 'year', 'offer', 'sets']\n",
            "\n",
            "Topic 59: ['nnst', 'american', 'offer', 'china', 'june']\n",
            "\n",
            "Topic 60: ['gas', 'tax', 'american', 'coffee', 'preferred']\n",
            "\n",
            "Topic 61: ['dividend', 'nnst', 'qtr', 'new', 'nil']\n",
            "\n",
            "Topic 62: ['corp', 'notes', 'rate', 'exchange', 'nnrd']\n",
            "\n",
            "Topic 63: ['mths', 'notes', 'nnrd', 'dollar', 'tax']\n",
            "\n",
            "Topic 64: ['china', 'nil', 'june', 'coffee', 'debentures']\n",
            "\n",
            "Topic 65: ['miles', 'load', 'tax', 'dividend', 'factor']\n",
            "\n",
            "Topic 66: ['gas', 'nil', 'sugar', 'brazil', 'split']\n",
            "\n",
            "Topic 67: ['group', 'week', 'american', 'months', 'contract']\n",
            "\n",
            "Topic 68: ['american', 'sugar', 'price', 'tax', 'nnp']\n",
            "\n",
            "Topic 69: ['march', 'sugar', 'credit', 'loan', 'split']\n",
            "\n",
            "Topic 70: ['corp', 'china', 'march', 'split', 'dlr']\n",
            "\n",
            "Topic 71: ['nil', 'deficit', 'dlr', 'group', 'trade']\n",
            "\n",
            "Topic 72: ['offer', 'computer', 'tax', 'debt', 'systems']\n",
            "\n",
            "Topic 73: ['credit', 'china', 'debt', 'tax', 'agreement']\n",
            "\n",
            "Topic 74: ['china', 'american', 'notes', 'net', 'says']\n",
            "\n",
            "Topic 75: ['months', 'cash', 'march', 'swiss', 'lme']\n",
            "\n",
            "Topic 76: ['agreement', 'union', 'soviet', 'quarter', 'nil']\n",
            "\n",
            "Topic 77: ['texaco', 'share', 'court', 'credit', 'union']\n",
            "\n",
            "Topic 78: ['quarter', 'credit', 'revs', 'months', 'texaco']\n",
            "\n",
            "Topic 79: ['notes', 'cocoa', 'gas', 'cash', 'corn']\n",
            "\n",
            "Topic 80: ['preferred', 'futures', 'american', 'split', 'credit']\n",
            "\n",
            "Topic 81: ['cocoa', 'banks', 'week', 'buffer', 'fund']\n",
            "\n",
            "Topic 82: ['corn', 'quarter', 'week', 'taiwan', 'march']\n",
            "\n",
            "Topic 83: ['notes', 'texaco', 'taiwan', 'computer', 'dlr']\n",
            "\n",
            "Topic 84: ['week', 'taiwan', 'index', 'june', 'money']\n",
            "\n",
            "Topic 85: ['notes', 'preferred', 'warrants', 'merger', 'opec']\n",
            "\n",
            "Topic 86: ['sale', 'gas', 'debentures', 'shares', 'kong']\n",
            "\n",
            "Topic 87: ['gas', 'index', 'nnh', 'nnl', 'march']\n",
            "\n",
            "Topic 88: ['sale', 'group', 'nnh', 'nnl', 'notes']\n",
            "\n",
            "Topic 89: ['futures', 'group', 'offer', 'taiwan', 'unit']\n",
            "\n",
            "Topic 90: ['south', 'cocoa', 'untrd', 'yen', 'money']\n",
            "\n",
            "Topic 91: ['soviet', 'dlr', 'bonds', 'prime', 'warrants']\n",
            "\n",
            "Topic 92: ['bills', 'february', 'sale', 'report', 'month']\n",
            "\n",
            "Topic 93: ['usair', 'banks', 'twa', 'deficit', 'texas']\n",
            "\n",
            "Topic 94: ['bonds', 'company', 'china', 'chrysler', 'shareholders']\n",
            "\n",
            "Topic 95: ['says', 'nnh', 'nnl', 'bonds', 'nna']\n",
            "\n",
            "Topic 96: ['brazil', 'cocoa', 'january', 'corn', 'days']\n",
            "\n",
            "Topic 97: ['chrysler', 'rates', 'brazil', 'split', 'tax']\n",
            "\n",
            "Topic 98: ['kong', 'chrysler', 'hong', 'south', 'west']\n",
            "\n",
            "Topic 99: ['general', 'gencorp', 'share', 'index', 'bonds']\n",
            "\n",
            "Topic 100: ['says', 'south', 'sets', 'rates', 'china']\n",
            "\n",
            "Topic 101: ['january', 'month', 'bills', 'december', 'warrants']\n",
            "\n",
            "Topic 102: ['kong', 'gm', 'hong', 'revs', 'stores']\n",
            "\n",
            "Topic 103: ['chrysler', 'soviet', 'taiwan', 'index', 'money']\n",
            "\n",
            "Topic 104: ['stores', 'quarterly', 'usair', 'department', 'rate']\n",
            "\n",
            "Topic 105: ['days', 'group', 'foreign', 'financial', 'gm']\n",
            "\n",
            "Topic 106: ['days', 'rose', 'unit', 'stores', 'canada']\n",
            "\n",
            "Topic 107: ['days', 'south', 'bond', 'africa', 'prime']\n",
            "\n",
            "Topic 108: ['national', 'share', 'official', 'earnings', 'mths']\n",
            "\n",
            "Topic 109: ['corn', 'plant', 'banks', 'systems', 'oil']\n",
            "\n",
            "Topic 110: ['bond', 'dmk', 'sets', 'rate', 'ffr']\n",
            "\n",
            "Topic 111: ['cocoa', 'wheat', 'days', 'industries', 'south']\n",
            "\n",
            "Topic 112: ['soviet', 'cyclops', 'sale', 'trading', 'market']\n",
            "\n",
            "Topic 113: ['chrysler', 'industries', 'kong', 'hong', 'warrants']\n",
            "\n",
            "Topic 114: ['quarterly', 'japanese', 'national', 'sale', 'swiss']\n",
            "\n",
            "Topic 115: ['industries', 'taiwan', 'merger', 'unit', 'soviet']\n",
            "\n",
            "Topic 116: ['drug', 'days', 'chrysler', 'unit', 'usair']\n",
            "\n",
            "Topic 117: ['days', 'bond', 'quarterly', 'sets', 'farm']\n",
            "\n",
            "Topic 118: ['usair', 'twa', 'week', 'unit', 'payout']\n",
            "\n",
            "Topic 119: ['drug', 'industries', 'usair', 'products', 'brazil']\n",
            "\n",
            "Topic 120: ['national', 'industries', 'usair', 'swiss', 'revs']\n",
            "\n",
            "Topic 121: ['air', 'chrysler', 'marks', 'taiwan', 'unch']\n",
            "\n",
            "Topic 122: ['texas', 'air', 'cyclops', 'says', 'products']\n",
            "\n",
            "Topic 123: ['systems', 'issue', 'capital', 'marks', 'venture']\n",
            "\n",
            "Topic 124: ['pacific', 'trust', 'security', 'index', 'gencorp']\n",
            "\n",
            "Topic 125: ['industries', 'international', 'program', 'company', 'money']\n",
            "\n",
            "Topic 126: ['nnp', 'program', 'talks', 'gas', 'energy']\n",
            "\n",
            "Topic 127: ['paper', 'crowns', 'board', 'loans', 'term']\n",
            "\n",
            "Topic 128: ['unch', 'systems', 'bonds', 'texaco', 'financial']\n",
            "\n",
            "Topic 129: ['pacific', 'stocks', 'industries', 'energy', 'los']\n",
            "\n",
            "Topic 130: ['bonds', 'prices', 'rubber', 'swiss', 'quarterly']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DDNsAJDFvzl",
        "outputId": "a8ece307-c7f0-4710-9348-84972925238d"
      },
      "source": [
        "len(lsa.components_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxzKOVe8Stkj"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Ox9GkIRC8d"
      },
      "source": [
        "Grid Search for best parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL5ImBxgY974"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rC1KMGcb_hl"
      },
      "source": [
        "# Splitting the data X and y into training and testing sets with the testing size\n",
        "X_train, X_test, y_train, y_test = train_test_split(raw_X, raw_y, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4CouwXrfY6x"
      },
      "source": [
        "assert len(X_train) == len(y_train)\n",
        "assert len(X_test) == len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl_wqu6QHT7H",
        "outputId": "3de0db0f-2050-4e7d-f083-ea8b8f05cc08"
      },
      "source": [
        "#Grid search\n",
        "\n",
        "#Hyperparameters \n",
        "components = [10, 20, 50, 100, 120, 150, 170, 200, 220, 250]\n",
        "value_neighbors = [3, 5, 7, 10]\n",
        "\n",
        "#dataframe\n",
        "results = []\n",
        "\n",
        "#vectorizer \n",
        "vectorizer = TfidfVectorizer(analyzer='word',\n",
        "                             stop_words='english', #filtering out stop words\n",
        "                             min_df=5, #ignore terms that have a document frequency lower than 5\n",
        "                             max_df=0.95, #ignore terms that appear in 95% of documents\n",
        "                             max_features=10000, #10000 most frequently appearing words\n",
        "                             strip_accents='ascii')\n",
        "\n",
        "#grid search\n",
        "for value in components:\n",
        "  for v in value_neighbors:\n",
        "\n",
        "    print(f'Training with hyperparameters: number of components [{value}], number of neigbors [{v}].')\n",
        "\n",
        "    svd = TruncatedSVD(n_components=value, random_state=1)\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    #X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    X_train_lsa = svd.fit_transform(X_train_vectorized)\n",
        "    #X_test_lsa = svd.transform(X_test_vectorized)\n",
        "\n",
        "    knn_lsa = KNeighborsClassifier(n_neighbors=v)\n",
        "    knn_lsa.fit(X_train_lsa, y_train)\n",
        "\n",
        "\n",
        "    predicted = knn_lsa.predict(X_train_lsa)\n",
        "\n",
        "    acc = (np.sum(predicted==y_train)/len(y_train)) *100\n",
        "           \n",
        "    #dataframe\n",
        "    results.append({\n",
        "                  'number of components': value,\n",
        "                  'number of neigbors': v,\n",
        "                  'correct labels': f'{right_labels}/{len(y_train)}',\n",
        "                  'Accuracy': (\"%.2f%%\" % (acc))\n",
        "                    })\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with hyperparameters: number of components [10], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [10], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [10], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [10], number of neigbors [10].\n",
            "Training with hyperparameters: number of components [20], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [20], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [20], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [20], number of neigbors [10].\n",
            "Training with hyperparameters: number of components [50], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [50], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [50], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [50], number of neigbors [10].\n",
            "Training with hyperparameters: number of components [100], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [100], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [100], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [100], number of neigbors [10].\n",
            "Training with hyperparameters: number of components [120], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [120], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [120], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [120], number of neigbors [10].\n",
            "Training with hyperparameters: number of components [150], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [150], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [150], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [150], number of neigbors [10].\n",
            "Training with hyperparameters: number of components [170], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [170], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [170], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [170], number of neigbors [10].\n",
            "Training with hyperparameters: number of components [200], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [200], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [200], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [200], number of neigbors [10].\n",
            "Training with hyperparameters: number of components [220], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [220], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [220], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [220], number of neigbors [10].\n",
            "Training with hyperparameters: number of components [250], number of neigbors [3].\n",
            "Training with hyperparameters: number of components [250], number of neigbors [5].\n",
            "Training with hyperparameters: number of components [250], number of neigbors [7].\n",
            "Training with hyperparameters: number of components [250], number of neigbors [10].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUhfF5JpT7hi",
        "outputId": "f0e56cc9-3a0a-4fbd-fb8d-d2f3786f4f93"
      },
      "source": [
        "import pandas as pd\n",
        "results = pd.DataFrame(results)\n",
        "results = results.sort_values(by='Accuracy', ascending=False)\n",
        "\n",
        "#table of hyperparameters\n",
        "#sorted by accuracy\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Results of training hyperparameters')\n",
        "print('------------------------------------------------------------------------')\n",
        "print(results)\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Results of training hyperparameters\n",
            "------------------------------------------------------------------------\n",
            "    number of components  number of neigbors correct labels Accuracy\n",
            "20                   150                   3     3450/17262   88.93%\n",
            "32                   220                   3     3450/17262   88.83%\n",
            "28                   200                   3     3450/17262   88.80%\n",
            "24                   170                   3     3450/17262   88.79%\n",
            "16                   120                   3     3450/17262   88.74%\n",
            "36                   250                   3     3450/17262   88.73%\n",
            "12                   100                   3     3450/17262   88.66%\n",
            "8                     50                   3     3450/17262   88.01%\n",
            "29                   200                   5     3450/17262   86.62%\n",
            "25                   170                   5     3450/17262   86.55%\n",
            "21                   150                   5     3450/17262   86.55%\n",
            "17                   120                   5     3450/17262   86.48%\n",
            "13                   100                   5     3450/17262   86.46%\n",
            "33                   220                   5     3450/17262   86.39%\n",
            "37                   250                   5     3450/17262   86.38%\n",
            "9                     50                   5     3450/17262   85.52%\n",
            "30                   200                   7     3450/17262   85.06%\n",
            "18                   120                   7     3450/17262   84.98%\n",
            "22                   150                   7     3450/17262   84.94%\n",
            "4                     20                   3     3450/17262   84.94%\n",
            "26                   170                   7     3450/17262   84.91%\n",
            "34                   220                   7     3450/17262   84.90%\n",
            "38                   250                   7     3450/17262   84.88%\n",
            "14                   100                   7     3450/17262   84.82%\n",
            "10                    50                   7     3450/17262   83.92%\n",
            "23                   150                  10     3450/17262   83.77%\n",
            "15                   100                  10     3450/17262   83.70%\n",
            "27                   170                  10     3450/17262   83.63%\n",
            "19                   120                  10     3450/17262   83.59%\n",
            "31                   200                  10     3450/17262   83.54%\n",
            "35                   220                  10     3450/17262   83.32%\n",
            "39                   250                  10     3450/17262   83.18%\n",
            "11                    50                  10     3450/17262   82.93%\n",
            "5                     20                   5     3450/17262   81.89%\n",
            "0                     10                   3     3450/17262   80.91%\n",
            "6                     20                   7     3450/17262   80.65%\n",
            "7                     20                  10     3450/17262   78.92%\n",
            "1                     10                   5     3450/17262   77.45%\n",
            "2                     10                   7     3450/17262   75.66%\n",
            "3                     10                  10     3450/17262   74.18%\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgBFmGXhbXpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6166b30-78bb-4b90-9e13-30d018a3d0cc"
      },
      "source": [
        "#Final model with the best hyperparameters\n",
        "\n",
        "svd = TruncatedSVD(n_components=150, random_state=1) \n",
        "vectorizer = TfidfVectorizer(analyzer='word',\n",
        "                             stop_words='english', #filtering out stop words\n",
        "                             min_df=5, #ignore terms that have a document frequency lower than 5\n",
        "                             max_df=0.95, #ignore terms that appear in 95% of documents\n",
        "                             max_features=10000, #10000 most frequently appearing words\n",
        "                             strip_accents='ascii')\n",
        "\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "X_train_lsa = svd.fit_transform(X_train_vectorized)\n",
        "X_test_lsa = svd.transform(X_test_vectorized)\n",
        "\n",
        "\n",
        "knn_lsa = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_lsa.fit(X_train_lsa, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKvpgqmkY6bt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6df59fd-2408-4ec2-a17a-98fc5107ff81"
      },
      "source": [
        "print('Predicting labels...')\n",
        "predicted_labels = knn_lsa.predict(X_test_lsa)\n",
        "\n",
        "test_accuracy = (np.sum(predicted_labels==y_test)/len(y_test)) *100\n",
        "print('----------------------------------------------------------------------')\n",
        "print(f'Test accuracy: {(\"%.4f%%\" % (test_accuracy))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels...\n",
            "----------------------------------------------------------------------\n",
            "Test accuracy: 80.8387%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB8GxM1JOii2"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6GhJ1pN-MxT"
      },
      "source": [
        "import warnings\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "  warnings.simplefilter(\"ignore\")\n",
        "  \n",
        "  X_vectorized = vectorizer.fit_transform(raw_X)\n",
        "\n",
        "  X_lsa = svd.fit_transform(X_vectorized)\n",
        "  \n",
        "  scores = cross_val_score(knn_lsa, X_lsa, raw_y, cv=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSp727k_BWgf",
        "outputId": "3aacb71c-e7bb-455d-db8d-7be6beab258d"
      },
      "source": [
        "#Crossvalidation score\n",
        "print('----------------------------------------------------------------------')\n",
        "print('Crossvalidation:')\n",
        "print(scores)\n",
        "print('----------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------\n",
            "Crossvalidation:\n",
            "[0.79170528 0.78568119 0.78081557 0.801854   0.80440324]\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc8XZKJPAZ_G",
        "outputId": "0de3cfb0-3f38-4bcb-a090-4d2ba53e7e26"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "  warnings.simplefilter(\"ignore\")\n",
        "  report = classification_report(y_test, predicted_labels)\n",
        "\n",
        "  \n",
        "print('----------------------------------------------------------------------')\n",
        "print('Classification report:')\n",
        "print('----------------------------------------------------------------------')\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------\n",
            "Classification report:\n",
            "----------------------------------------------------------------------\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "                      0.82      0.90      0.85      2058\n",
            "            acq       0.75      0.72      0.73       442\n",
            "           alum       1.00      0.27      0.43        11\n",
            "            bop       0.50      0.40      0.44        10\n",
            "        carcass       0.25      0.20      0.22         5\n",
            "          cocoa       0.92      1.00      0.96        12\n",
            "         coffee       0.88      1.00      0.94        23\n",
            "         copper       1.00      0.60      0.75        10\n",
            "           corn       0.00      0.00      0.00         1\n",
            "         cotton       0.00      0.00      0.00         7\n",
            "            cpi       0.62      0.48      0.54        21\n",
            "            cpu       0.00      0.00      0.00         3\n",
            "          crude       0.83      0.79      0.81       108\n",
            "            dlr       0.50      0.17      0.25        12\n",
            "           earn       0.91      0.89      0.90       804\n",
            "       f-cattle       0.00      0.00      0.00         0\n",
            "       fishmeal       0.00      0.00      0.00         1\n",
            "           fuel       0.50      0.50      0.50         2\n",
            "            gas       0.00      0.00      0.00         8\n",
            "            gnp       0.58      0.50      0.54        30\n",
            "           gold       0.74      0.77      0.75        26\n",
            "          grain       0.86      0.86      0.86       107\n",
            "      groundnut       0.00      0.00      0.00         1\n",
            "           heat       1.00      1.00      1.00         2\n",
            "            hog       0.50      0.50      0.50         2\n",
            "        housing       0.67      0.40      0.50         5\n",
            "         income       0.00      0.00      0.00         2\n",
            "    instal-debt       0.00      0.00      0.00         1\n",
            "       interest       0.66      0.57      0.61        65\n",
            "    inventories       0.00      0.00      0.00         1\n",
            "            ipi       0.80      0.73      0.76        11\n",
            "     iron-steel       0.50      0.30      0.37        10\n",
            "            jet       0.00      0.00      0.00         1\n",
            "           jobs       0.57      0.50      0.53         8\n",
            "           lead       0.50      0.50      0.50         4\n",
            "            lei       0.33      0.14      0.20         7\n",
            "      livestock       0.50      0.25      0.33        16\n",
            "         lumber       0.00      0.00      0.00         2\n",
            "      meal-feed       0.00      0.00      0.00         2\n",
            "       money-fx       0.68      0.66      0.67       139\n",
            "   money-supply       0.76      0.85      0.80        26\n",
            "        nat-gas       0.67      0.31      0.42        13\n",
            "         nickel       0.00      0.00      0.00         1\n",
            "        oilseed       0.67      0.53      0.59        15\n",
            "         orange       0.80      0.80      0.80         5\n",
            "       palm-oil       0.00      0.00      0.00         1\n",
            "       pet-chem       0.00      0.00      0.00         4\n",
            "         potato       0.50      0.50      0.50         2\n",
            "        propane       0.00      0.00      0.00         1\n",
            "       rapeseed       0.00      0.00      0.00         1\n",
            "       reserves       0.83      0.25      0.38        20\n",
            "         retail       1.00      0.40      0.57         5\n",
            "         rubber       0.75      0.60      0.67         5\n",
            "           ship       0.60      0.56      0.58        50\n",
            "         silver       1.00      0.17      0.29         6\n",
            "            stg       0.00      0.00      0.00         1\n",
            "strategic-metal       0.50      0.17      0.25         6\n",
            "          sugar       1.00      0.78      0.88        27\n",
            "            tea       0.50      1.00      0.67         1\n",
            "            tin       0.25      0.20      0.22         5\n",
            "          trade       0.79      0.63      0.70       115\n",
            "        veg-oil       0.82      0.53      0.64        17\n",
            "          wheat       0.00      0.00      0.00         5\n",
            "           wool       0.00      0.00      0.00         1\n",
            "            wpi       0.17      0.33      0.22         3\n",
            "           zinc       0.00      0.00      0.00         0\n",
            "\n",
            "       accuracy                           0.81      4316\n",
            "      macro avg       0.45      0.37      0.39      4316\n",
            "   weighted avg       0.80      0.81      0.80      4316\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmATcv66giVh"
      },
      "source": [
        "## Baseline model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp6X9ZkGgmIS"
      },
      "source": [
        "#simple baseline model\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_baseline= vectorizer.fit_transform(X_train)\n",
        "X_test_baseline= vectorizer.transform(X_test)\n",
        "\n",
        "knn_baseline = KNeighborsClassifier()\n",
        "knn_baseline.fit(X_train_baseline, y_train)\n",
        "\n",
        "#predict\n",
        "\n",
        "baseline_preds = knn_baseline.predict(X_test_baseline)\n",
        "\n",
        "#accuracy\n",
        "\n",
        "baseline_accuracy = (np.sum(baseline_preds==y_test)/len(y_test)) *100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKM2txI3hamm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd3bed0-ce73-4cb6-963f-54cf29e3cb92"
      },
      "source": [
        "print('----------------------------------------------------------------------')\n",
        "print(f'Test accuracy: {(\"%.2f%%\" % (baseline_accuracy))}')\n",
        "print('----------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------\n",
            "Test accuracy: 77.94%\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}